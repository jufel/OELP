# -*- coding: utf-8 -*-
"""OELP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16bRGYAVpAHUQwd3DIBJA5y2zGbi56Z5q
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import pprint
# %tensorflow_version 1.x
import tensorflow as tf
import random
random.seed( 30 )
if 'COLAB_TPU_ADDR' not in os.environ:
  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')
else:
  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print ('TPU address is', tpu_address)

  with tf.Session(tpu_address) as session:
    devices = session.list_devices()
    
  print('TPU devices:')
  pprint.pprint(devices)

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals

try:
  # The %tensorflow_version magic only works in colab.
#   %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf

import os
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras as keras
from scipy.io import loadmat

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/My\ Drive/NEW\ OELP\ DATA/New\ Data/16MPSK

print(len(os.listdir('/content/drive/My Drive/NEW OELP DATA/New Data/16MPSK')))

DataDir = "/content/drive/My Drive/NEW OELP DATA/New Data"
CATEGORIES = ['16MPSK',	'16MQAM',	'2MPSK',  '4MQAM',  '64MPSK',  '64MQAM',  '8MPSK']

for category in CATEGORIES:
    path = os.path.join(DataDir,category)
    for data in os.listdir(path):
        datum=loadmat(os.path.join(path,data))
        break
    break

print(os.path.join(path,data))

import matplotlib.pyplot as plt 
import cmath
import numpy as np

a = np.array(datum['receivedSignal'])
print(a.shape)

X = [a.real]
Y = [a.imag]
plt.scatter(X,Y, color='red')
plt.show()

training_data=[]

def create_train_data():
    j = 0 ;
    SNR = [-1,-2,0,1,10,11,12,13,14,15,2,3,4,5,6,7,8,9];
    print(j)
    for category in CATEGORIES:
        path = os.path.join(DataDir,category)
        class_num = CATEGORIES.index(category)
        i=0
        for data in os.listdir(path):
            try:
                if i == 18:
                  i=0
                print("j1 = ", j)
                datum=loadmat(os.path.join(path,data))
                a = np.array(datum['receivedSignal'])
                training_data.append([a, class_num,SNR[i]])
                print('loading ',os.path.join(path,data))
                print('SNR ',SNR[i])
                i=i+1
                j=j+1
                # print("j2 = ",j)
            except Exception as e:
                pass
        
create_train_data()

print(np.shape(training_data))
print(training_data[0])

X = []
y = []
snr=[]
depth = 7
indices = [0, 1, 2, 3]
for features, label, s in training_data:
    X.append(features)
    y.append(label)
    snr.append(s)
  
X = np.array(X).reshape(-1,1000000)
y1 = np.array(y).reshape(-1,1)
snr = np.array(snr).reshape(-1,1)

from numpy import argmax
from keras.utils import to_categorical

# one hot encode
y2 = to_categorical(y1)

print('Shape of X ',np.shape(X))
print('Shape of y ',np.shape(y2))
print('Shape pf snr ',np.shape(snr))

print(snr[7])

input_size=1000
print(X[0][input_size])
n = 126*int(1000000/input_size)
x = np.reshape(X,(n,int(input_size)))
print(x[1][0],x.shape)

train_data=np.zeros((n,input_size,2))         # 2 is to split into real and imag 
labels=np.zeros((n,7))            # labels are according to the order of 1.'16MPSK',	2.'16MQAM',	3.'2MPSK',  4.'4MQAM',  5.'64MPSK',  6.'64MQAM',  7.'8MPSK'
snr_labels=np.zeros((n,1))
for i in range(n):
  index = int(np.floor(i/(n/7)))
  labels[i][index] = 1
  
  snr_labels[i] = snr[int((i/(1000000/input_size)))]
  for j in range(input_size):
    train_data[i][j][0] = x[i][j].real
    train_data[i][j][1] = x[i][j].imag

print(i)
print(n)
print(snr_labels[0])

unique, counts = np.unique(snr_labels, return_counts=True)
dict(zip(unique, counts))

import pandas as pd
from sklearn.model_selection import train_test_split

X_minus1db = []
X_minus2db = []
X_0db = []
X_1db = []
X_2db = []
X_3db = []
X_4db=[]
X_5db=[]
X_6db=[]
X_7db=[]
X_8db=[]
X_9db = []
X_10db=[]
X_11db = []
X_12db=[]
X_13db = []
X_14db = []
X_15db=[]

y_minus1db = []
y_minus2db = []
y_0db = []
y_1db = []
y_2db = []
y_3db = []
y_4db=[]
y_4db=[]
y_5db=[]
y_6db=[]
y_7db=[]
y_8db=[]
y_9db = []
y_10db=[]
y_11db = []
y_12db=[]
y_13db = []
y_14db = []
y_15db=[]

for j in range(len(train_data)):
    if snr_labels[j]==-2:
      X_minus2db.append(train_data[j])
      y_minus2db.append(labels[j])
    if snr_labels[j]==-1:
      X_minus1db.append(train_data[j])
      y_minus1db.append(labels[j])
    if snr_labels[j]==0:
      X_0db.append(train_data[j])
      y_0db.append(labels[j])
    if snr_labels[j]==1:
      X_1db.append(train_data[j])
      y_1db.append(labels[j])
    if snr_labels[j]==2:
      X_2db.append(train_data[j])
      y_2db.append(labels[j])
    if snr_labels[j]==3:
      X_3db.append(train_data[j])
      y_3db.append(labels[j])
    if snr_labels[j]==4:
      X_4db.append(train_data[j])
      y_4db.append(labels[j])
    if snr_labels[j]==5:
      X_5db.append(train_data[j])
      y_5db.append(labels[j])
    if snr_labels[j]==6:
      X_6db.append(train_data[j])
      y_6db.append(labels[j])
    if snr_labels[j]==7:
      X_7db.append(train_data[j])
      y_7db.append(labels[j])
    if snr_labels[j]==8:
      X_8db.append(train_data[j])
      y_8db.append(labels[j])
    if snr_labels[j]==9:
      X_9db.append(train_data[j])
      y_9db.append(labels[j])
    if snr_labels[j]==10:
      X_10db.append(train_data[j])
      y_10db.append(labels[j])
    if snr_labels[j]==11:
      X_11db.append(train_data[j])
      y_11db.append(labels[j])
    if snr_labels[j]==12:
      X_12db.append(train_data[j])
      y_12db.append(labels[j])
    if snr_labels[j]==13:
      X_13db.append(train_data[j])
      y_13db.append(labels[j])
    if snr_labels[j]==14:
      X_14db.append(train_data[j])
      y_14db.append(labels[j])
    if snr_labels[j]==15:
      X_15db.append(train_data[j])
      y_15db.append(labels[j])

X_minus2db=np.array(X_minus2db)
X_minus1db=np.array(X_minus1db)
X_0db=np.array(X_0db)
X_1db=np.array(X_1db)
X_2db=np.array(X_2db)
X_3db=np.array(X_3db)
X_4db=np.array(X_4db)
X_5db=np.array(X_5db)
X_6db=np.array(X_6db)
X_7db=np.array(X_7db)
X_8db=np.array(X_8db)
X_9db=np.array(X_9db)
X_10db=np.array(X_10db)
X_11db=np.array(X_11db)
X_12db=np.array(X_12db)
X_13db=np.array(X_13db)
X_14db=np.array(X_14db)
X_15db=np.array(X_15db)

y_minus2db=np.array(y_minus2db)
y_minus1db=np.array(y_minus1db)
y_0db=np.array(y_0db)
y_1db=np.array(y_1db)
y_2db=np.array(y_2db)
y_3db=np.array(y_3db)
y_4db=np.array(y_4db)
y_5db=np.array(y_5db)
y_6db=np.array(y_6db)
y_7db=np.array(y_7db)
y_8db=np.array(y_8db)
y_9db=np.array(y_9db)
y_10db=np.array(y_10db)
y_11db=np.array(y_11db)
y_12db=np.array(y_12db)
y_13db=np.array(y_13db)
y_14db=np.array(y_14db)
y_15db=np.array(y_15db)

X_trainminus2db,X_testminus2db,y_trainminus2db,y_testminus2db=train_test_split(X_minus2db,y_minus2db,test_size=0.05)
X_trainminus1db,X_testminus1db,y_trainminus1db,y_testminus1db=train_test_split(X_minus1db,y_minus1db,test_size=0.05)
X_train0db,X_test0db,y_train0db,y_test0db=train_test_split(X_0db,y_0db,test_size=0.05)
X_train1db,X_test1db,y_train1db,y_test1db=train_test_split(X_1db,y_1db,test_size=0.05)
X_train2db,X_test2db,y_train2db,y_test2db=train_test_split(X_2db,y_2db,test_size=0.05)
X_train3db,X_test3db,y_train3db,y_test3db=train_test_split(X_3db,y_3db,test_size=0.05)
X_train4db,X_test4db,y_train4db,y_test4db=train_test_split(X_4db,y_4db,test_size=0.05)
X_train5db,X_test5db,y_train5db,y_test5db=train_test_split(X_5db,y_5db,test_size=0.05)
X_train6db,X_test6db,y_train6db,y_test6db=train_test_split(X_6db,y_6db,test_size=0.05)
X_train7db,X_test7db,y_train7db,y_test7db=train_test_split(X_7db,y_7db,test_size=0.05)
X_train8db,X_test8db,y_train8db,y_test8db=train_test_split(X_8db,y_8db,test_size=0.05)
X_train9db,X_test9db,y_train9db,y_test9db=train_test_split(X_9db,y_9db,test_size=0.05)
X_train10db,X_test10db,y_train10db,y_test10db=train_test_split(X_10db,y_10db,test_size=0.05)
X_train11db,X_test11db,y_train11db,y_test11db=train_test_split(X_11db,y_11db,test_size=0.05)
X_train12db,X_test12db,y_train12db,y_test12db=train_test_split(X_12db,y_12db,test_size=0.05)
X_train13db,X_test13db,y_train13db,y_test13db=train_test_split(X_13db,y_13db,test_size=0.05)
X_train14db,X_test14db,y_train14db,y_test14db=train_test_split(X_14db,y_14db,test_size=0.05)
X_train15db,X_test15db,y_train15db,y_test15db=train_test_split(X_15db,y_15db,test_size=0.05)

print(np.shape(X_train4db), np.shape(y_train4db))
print(np.shape(X_test4db), np.shape(y_test4db))

X_train = np.concatenate((X_trainminus2db,X_trainminus1db,X_train0db,X_train1db,X_train2db,X_train3db,X_train4db,X_train5db,X_train6db,X_train7db,
                         X_train8db,X_train9db,X_train10db,X_train11db,X_train12db,X_train13db,X_train14db,X_train15db),axis=0)
y_train = np.concatenate((y_trainminus2db,y_trainminus1db,y_train0db,y_train1db,y_train2db,y_train3db,y_train4db,y_train5db,y_train6db,y_train7db,
                         y_train8db,y_train9db,y_train10db,y_train11db,y_train12db,y_train13db,y_train14db,y_train15db),axis=0)
X_test = np.concatenate((X_testminus2db,X_testminus1db,X_test0db,X_test1db,X_test2db,X_test3db,X_test4db,X_test5db,X_test6db,X_test7db,
                         X_test8db,X_test9db,X_test10db,X_test11db,X_test12db,X_test13db,X_test14db,X_test15db),axis=0)
y_test = np.concatenate((y_testminus2db,y_testminus1db,y_test0db,y_test1db,y_test2db,y_test3db,y_test4db,y_test5db,y_test6db,y_test7db,
                         y_test8db,y_test9db,y_test10db,y_test11db,y_test12db,y_test13db,y_test14db,y_test15db),axis=0)

mapIndexPosition = list(zip(X_train, y_train))
random.shuffle(mapIndexPosition)
X_train, y_train = zip(*mapIndexPosition)
X_train = np.asarray(X_train)
y_train = np.asarray(y_train)

print(np.shape(X_train),np.shape(y_train))

print(y_train[107][:])

X_train = np.expand_dims(X_train, axis=3)
X_test = np.expand_dims(X_test, axis=3)
#y_train = np.expand_dims(y_train, axis=3)
#y_test = np.expand_dims(y_test, axis=3)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input
from tensorflow.keras import regularizers

import numpy as np
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing import image
#from tensorflow.keras.utils import layer_utils
#from tensorflow.keras.utils.data_utils import get_file
from tensorflow.keras.applications.imagenet_utils import preprocess_input
import pydot
from IPython.display import SVG
#from tensorflow.keras.utils.vis_utils import model_to_dot
#from tensorflow.keras.utils import plot_model
#from resnets_utils import *
from tensorflow.keras.initializers import glorot_uniform
import scipy.misc
from matplotlib.pyplot import imshow

def identity_block(X, filters, stage, block):
    """
    Implementation of the identity block as defined in Figure 3
    
    Arguments:
    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
    f -- integer, specifying the shape of the middle CONV's window for the main path
    filters -- python list of integers, defining the number of filters in the CONV layers of the main path
    stage -- integer, used to name the layers, depending on their position in the network
    block -- string/character, used to name the layers, depending on their position in the network
    
    Returns:
    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)
    """
    
    # defining name basis
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    
    # Retrieve Filters
    F1, F2 = filters
    
    # Save the input value. You'll need this later to add back to the main path. 
    X_shortcut = X
    
    # First component of main path
    X = Conv2D(filters = F1, kernel_size = (5, 1), strides = (1,1), padding = 'same', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)
    X = Activation('relu')(X)
    


    # Second component of main path
    X = Conv2D(filters = F2, kernel_size = (5, 1), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)

    # Final step: Add shortcut value to main path, and pass it through a RELU activation 
    X = Add()([X, X_shortcut])
    X = Activation('relu')(X)
    
    ### END CODE HERE ###
    
    return X

def ResNet(input_shape, classes = 7):
    """
   
    Arguments:
    input_shape -- shape of the images of the dataset
    classes -- integer, number of classes

    Returns:
    model -- a Model() instance in Keras
    """
    
    # Define the input as a tensor with shape input_shape
    X_input = Input(input_shape)

    
    X = ZeroPadding2D(1)(X_input)
    
    # Stage 1
    X = Conv2D(64, (5, 2), strides = (2, 1), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)
    X = Activation('relu')(X)
    X = MaxPooling2D((3, 1), strides=(2, 1))(X)

    # Stage 2
    X = identity_block(X, [64, 64], stage=2, block='a')
    X = MaxPooling2D((3, 1), strides=(2, 1))(X)
    X = identity_block(X, [64, 64], stage=2, block='b')

    X = AveragePooling2D(pool_size = (2, 2), name = 'avg_pool')(X)
    
   

    # output layer
    X = Flatten()(X)
    
    X = Dense(256, activation='relu', name='fc/256', kernel_initializer = glorot_uniform(seed=0))(X)
    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)

    
    # Create model
    model = Model(inputs = X_input, outputs = X, name='ResNet')

    return model

model = ResNet(input_shape = (input_size, 2, 1), classes = 7)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

model.summary()

checkpoint_path = "training_newdata_1000/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

model.fit(X_train, y_train, epochs = 20, batch_size = 128, callbacks=[cp_callback])

model.save('/content/drive/My Drive/OELP_data/resnet_New1000.h5')

model=tf.keras.models.load_model(
    '/content/drive/My Drive/OELP_data/resnet_New1000.h5',
    custom_objects=None,
    compile=True
)

#model.load_weights(checkpoint_path)
loss,acc = model.evaluate(X_test, y_test)
print("Restored model, accuracy: {:5.2f}%".format(100*acc))

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

#Confution Matrix and Classification Report
Y_pred = model.predict(X_test)
y_pred = np.argmax(Y_pred, axis=1)
y_test1 = np.argmax(y_test, axis=1)

print('Confusion Matrix')
print(confusion_matrix(y_test1, y_pred))
print('Classification Report')
print(classification_report(y_test1, y_pred, target_names=CATEGORIES))

print(acc)

"""Xtest_4db=[]
Xtest_5db=[]
Xtest_6db=[]
Xtest_7db=[]
Xtest_8db=[]
Xtest_10db=[]
Xtest_12db=[]
Xtest_15db=[]

ytest_4db=[]
ytest_5db=[]
ytest_6db=[]
ytest_7db=[]
ytest_8db=[]
ytest_10db=[]
ytest_12db=[]
ytest_15db=[]

for j in range(len(y_test)):
    if snr_test[j]==4:
      Xtest_4db.append(X_test[j])
      ytest_4db.append(y_test[j])
    if snr_test[j]==5:
      Xtest_5db.append(X_test[j])
      ytest_5db.append(y_test[j])
    if snr_test[j]==6:
      Xtest_6db.append(X_test[j])
      ytest_6db.append(y_test[j])
    if snr_test[j]==7:
      Xtest_7db.append(X_test[j])
      ytest_7db.append(y_test[j])
    if snr_test[j]==8:
      Xtest_8db.append(X_test[j])
      ytest_8db.append(y_test[j])
    if snr_test[j]==10:
      Xtest_10db.append(X_test[j])
      ytest_10db.append(y_test[j])
    if snr_test[j]==12:
      Xtest_12db.append(X_test[j])
      ytest_12db.append(y_test[j])
    if snr_test[j]==15:
      Xtest_15db.append(X_test[j])
      ytest_15db.append(y_test[j])"""


X_testminus2db=np.array(X_testminus2db)
X_testminus1db=np.array(X_testminus1db)
X_test0db=np.array(X_test0db)
X_test1db=np.array(X_test1db)
X_test2db=np.array(X_test2db)
X_test3db=np.array(X_test3db)
X_test4db=np.array(X_test4db)
X_test5db=np.array(X_test5db)
X_test6db=np.array(X_test6db)
X_test7db=np.array(X_test7db)
X_test8db=np.array(X_test8db)
X_test9db=np.array(X_test9db)
X_test10db=np.array(X_test10db)
X_test11db=np.array(X_test11db)
X_test12db=np.array(X_test12db)
X_test13db=np.array(X_test13db)
X_test14db=np.array(X_test14db)
X_test15db=np.array(X_test15db)

X_testminus2db = np.expand_dims(X_testminus2db, axis=3)
X_testminus1db = np.expand_dims(X_testminus1db, axis=3)
X_test0db=np.expand_dims(X_test0db, axis=3)
X_test1db=np.expand_dims(X_test1db, axis=3)
X_test2db=np.expand_dims(X_test2db, axis=3)
X_test3db=np.expand_dims(X_test3db, axis=3)
X_test4db=np.expand_dims(X_test4db, axis=3)
X_test5db=np.expand_dims(X_test5db, axis=3)
X_test6db=np.expand_dims(X_test6db, axis=3)
X_test7db=np.expand_dims(X_test7db, axis=3)
X_test8db=np.expand_dims(X_test8db, axis=3)
X_test9db=np.expand_dims(X_test9db, axis=3)
X_test10db=np.expand_dims(X_test10db, axis=3)
X_test11db=np.expand_dims(X_test11db, axis=3)
X_test12db=np.expand_dims(X_test12db, axis=3)
X_test13db=np.expand_dims(X_test13db, axis=3)
X_test14db=np.expand_dims(X_test14db, axis=3)
X_test15db=np.expand_dims(X_test15db, axis=3)

y_testminus2db=np.array(y_testminus2db)
y_testminus1db=np.array(y_testminus1db)
y_test0db=np.array(y_test0db)
y_test1db=np.array(y_test1db)
y_test2db=np.array(y_test2db)
y_test3db=np.array(y_test3db)
y_test4db=np.array(y_test4db)
y_test5db=np.array(y_test5db)
y_test6db=np.array(y_test6db)
y_test7db=np.array(y_test7db)
y_test8db=np.array(y_test8db)
y_test9db=np.array(y_test9db)
y_test10db=np.array(y_test10db)
y_test11db=np.array(y_test11db)
y_test12db=np.array(y_test12db)
y_test13db=np.array(y_test13db)
y_test14db=np.array(y_test14db)
y_test15db=np.array(y_test15db)

print(X_test8db.shape)
print(y_test8db.shape)

X_testminus2db = np.squeeze(X_testminus2db, axis=4)
X_testminus1db = np.squeeze(X_testminus1db, axis=4)
X_test0db=np.squeeze(X_test0db, axis=4)
X_test1db=np.squeeze(X_test1db, axis=4)
X_test2db=np.squeeze(X_test2db, axis=4)
X_test3db=np.squeeze(X_test3db, axis=4)
X_test4db=np.squeeze(X_test4db, axis=4)
X_test5db=np.squeeze(X_test5db, axis=4)
X_test6db=np.squeeze(X_test6db, axis=4)
X_test7db=np.squeeze(X_test7db, axis=4)
X_test8db=np.squeeze(X_test8db, axis=4)
X_test9db=np.squeeze(X_test9db, axis=4)
X_test10db=np.squeeze(X_test10db, axis=4)
X_test11db=np.squeeze(X_test11db, axis=4)
X_test12db=np.squeeze(X_test12db, axis=4)
X_test13db=np.squeeze(X_test13db, axis=4)
X_test14db=np.squeeze(X_test14db, axis=4)
X_test15db=np.squeeze(X_test15db, axis=4)

model=tf.keras.models.load_model(
    '/content/drive/My Drive/OELP_data/resnet_New1000.h5',
    custom_objects=None,
    compile=True
)

#model.load_weights(checkpoint_path)
lossminus2,accminus2 = model.evaluate(X_testminus2db, y_testminus2db)
lossminus1,accminus1 = model.evaluate(X_testminus1db, y_testminus1db)
loss0,acc0 = model.evaluate(X_test0db, y_test0db)
loss1,acc1 = model.evaluate(X_test1db, y_test1db)
loss2,acc2 = model.evaluate(X_test2db, y_test2db)
loss3,acc3 = model.evaluate(X_test3db, y_test3db)
loss4,acc4 = model.evaluate(X_test4db, y_test4db)
loss5,acc5 = model.evaluate(X_test5db, y_test5db)
loss6,acc6 = model.evaluate(X_test6db, y_test6db)
loss7,acc7 = model.evaluate(X_test7db, y_test7db)
loss8,acc8 = model.evaluate(X_test8db, y_test8db)
loss9,acc9 = model.evaluate(X_test9db, y_test9db)

loss10,acc10 = model.evaluate(X_test10db, y_test10db)
loss11,acc11 = model.evaluate(X_test11db, y_test11db)
loss12,acc12 = model.evaluate(X_test12db, y_test12db)
loss13,acc13 = model.evaluate(X_test13db, y_test13db)
loss14,acc14 = model.evaluate(X_test14db, y_test14db)
loss15,acc15 = model.evaluate(X_test15db, y_test15db)

ACC = [accminus2, accminus1, acc0,acc1,acc2,acc3,acc4,acc5,acc6,acc7,acc8,acc9,acc10,acc11,acc12,acc13,acc14,acc15]
SNR = [i for i in range(-2,16)]
plt.plot(SNR, ACC)
plt.grid()
plt.xlabel('SNR')
plt.ylabel('Accuracy')
#plt.annotate('data',xy=(SNR,ACC))

Y_pred = model.predict(X_testminus2db)
accminus2[0],accminus2[1],accminus2[2],accminus2[3],accminus2[4],accminus2[5],accminus2[6] = accuracy_score(y_testminus2db, Y_pred)
Y_pred = model.predict(X_testminus1db)
accminus1db[0],accminus1db[1],accminus1db[2],accminus1db[3],accminus1db[4],accminus1db[5],accminus1db[6] = accuracy_score(y_testminus1db, Y_pred)
Y_pred = model.predict(X_test0db)
acc0[0],acc0[1],acc0[2],acc0[3],acc0[4],acc0[5],acc0[6] = accuracy_score(y_test0db, Y_pred)
Y_pred = model.predict(X_test1db)
acc1[0],acc1[1],acc1[2],acc1[3],acc1[4],acc1[5],acc1[6] = accuracy_score(y_test1db, Y_pred)
Y_pred = model.predict(X_test2db)
acc2[0],acc2[1],acc2[2],acc2[3],acc2[4],acc2[5],acc2[6] = accuracy_score(y_test2db, Y_pred)
Y_pred = model.predict(X_test3db)
acc3[0],acc3[1],acc3[2],acc3[3],acc3[4],acc3[5],acc3[6] = accuracy_score(y_test3db, Y_pred)
Y_pred = model.predict(X_test4db)
acc4[0],acc4[1],acc4[2],acc4[3],acc4[4],acc4[5],acc4[6] = accuracy_score(y_test4db, Y_pred)
Y_pred = model.predict(X_test5db)
acc5[0],acc5[1],acc5[2],acc5[3],acc5[4],acc5[5],acc1[6] = accuracy_score(y_test5db, Y_pred)
Y_pred = model.predict(X_test6db)
acc6[0],acc6[1],acc6[2],acc6[3],acc6[4],acc6[5],acc6[6] = accuracy_score(y_test6db, Y_pred)
Y_pred = model.predict(X_test7db)
acc7[0],acc7[1],acc7[2],acc7[3],acc7[4],acc7[5],acc7[6] = accuracy_score(y_test7db, Y_pred)
Y_pred = model.predict(X_test8db)
acc8[0],acc8[1],acc8[2],acc8[3],acc8[4],acc8[5],acc8[6] = accuracy_score(y_test8db, Y_pred)
Y_pred = model.predict(X_test9db)
acc9[0],acc9[1],acc9[2],acc9[3],acc9[4],acc9[5],acc9[6] = accuracy_score(y_test9db, Y_pred)
Y_pred = model.predict(X_test10db)
acc10[0],acc10[1],acc10[2],acc10[3],acc10[4],acc10[5],acc10[6] = accuracy_score(y_test10db, Y_pred)
Y_pred = model.predict(X_test11db)
acc11[0],acc11[1],acc11[2],acc11[3],acc11[4],acc11[5],acc11[6] = accuracy_score(y_test11db, Y_pred)
Y_pred = model.predict(X_test12db)
acc12[0],acc12[1],acc12[2],acc12[3],acc12[4],acc12[5],acc12[6] = accuracy_score(y_test12db, Y_pred)
Y_pred = model.predict(X_test13db)
acc13[0],acc13[1],acc13[2],acc13[3],acc13[4],acc13[5],acc13[6] = accuracy_score(y_test13db, Y_pred)
Y_pred = model.predict(X_test14db)
acc14[0],acc14[1],acc14[2],acc14[3],acc14[4],acc14[5],acc14[6] = accuracy_score(y_test14db, Y_pred)
Y_pred = model.predict(X_test15db)
acc15[0],acc15[1],acc15[2],acc15[3],acc15[4],acc15[5],acc15[6] = accuracy_score(y_test15db, Y_pred)

Y_pred = model.predict(Xtest_7db)
y_pred = np.argmax(Y_pred, axis=1)
y_test1 = np.argmax(ytest_7db, axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_test1, y_pred))
print('Classification Report')
#target_names = ['Cats', 'Dogs', 'Horse']
print(classification_report(y_test1, y_pred, target_names=CATEGORIES))



no=[1000,800,500,400,250,200,100,50]
accu=[]
for k in no:
  X = []
  y = []
  snr=[]
  depth = 4
  indices = [0, 1, 2, 3]
  for features, label, s in training_data:
    X.append(features)
    y.append(label)
    snr.append(s)
  
  X = np.array(X).reshape(-1,500000)
  y1 = np.array(y).reshape(-1,1)
  snr = np.array(snr).reshape(-1,1)

# one hot encode
  y2 = to_categorical(y1)
  n = 32*int(500000/k)
  x = np.reshape(X,(n,int(k)))

  
  train_data=np.zeros((n,k,2))
  labels=np.zeros((n,4))
  snr_labels=np.zeros((n,1))
  for i in range(n):
    index = int(np.floor(i/(n/4)))
    labels[i][index] = 1
    snr_labels[i] = snr[int(np.floor(i/(500000/k)))]
    for j in range(k):
      train_data[i][j][0] = x[i][j].real
      train_data[i][j][1] = x[i][j].imag

  X_train,X_test,y_train,y_test,snr_train,snr_test=train_test_split(train_data,labels,snr_labels,test_size=0.05)

  print(X_train.shape, y_train.shape)
  print(X_test.shape, y_test.shape)
  print(snr_train.shape,snr_test.shape)

  X_train = np.expand_dims(X_train, axis=3)
  X_test = np.expand_dims(X_test, axis=3)

  print(X_train.shape, y_train.shape)
  print(X_test.shape, y_test.shape)



  model = ResNet(input_shape = (k, 2, 1), classes = 4)
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
  model.fit(X_train, y_train, epochs = 20, batch_size = 32, callbacks=[cp_callback])

  model.save("/content/drive/My Drive/OELP_data/Models for different input length/resnet_1_"+str(k)+".h5")

  loss,acc = model.evaluate(X_test, y_test)
  print("Restored model, accuracy: {:5.2f}%".format(100*acc))

  accu.append(100*acc)

print(accu)

k=0
print("/content/drive/My Drive/OELP_data/resnet_1_"+str(k)+".h5")

